{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the tweepy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy\n",
    "# !pip install mysqlclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import re\n",
    "\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import MySQLdb\n",
    "\n",
    "\n",
    "# For sentiment analysis of tweets\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tweeter Consumer Keys and Access Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'NFIO7bIgwQpdCZ7p7rgewKXss'\n",
    "consumer_secret= '2QegNrIpcGCW6nCWFTL5JEsrDCmYZpQHJ8SdIT7V0ql5cQgFwV'\n",
    "access_token= '3049227557-36ln3qak9xbxHvgNos45cf3uScGxuUdaDAR4vqs'\n",
    "access_token_secret= 'pZ4Y0p31graKrAYAy41mYztvIrYFzof7awiTDWQr2ui9u'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize AuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define search words and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"#Covid\"\n",
    "date_since = \"2018-11-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search, q=search_words, lang=\"en\", since=date_since).items(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tweepy.cursor.ItemIterator at 0x1363c7a83c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through tweets using for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 17:03:03\n",
      "2020-12-27 18:48:37\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from email.utils import parsedate_tz\n",
    "\n",
    "s='Mon Dec 07 17:03:03 +0000 2020'\n",
    "def to_datetime(datestring):\n",
    "    time_tuple = parsedate_tz(datestring.strip())\n",
    "    dt = datetime(*time_tuple[:6])\n",
    "    proper_date=(dt - timedelta(seconds=time_tuple[-1]))\n",
    "    return proper_date\n",
    "\n",
    "print(to_datetime(s))\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve tweets as a collection of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @RobertKennedyJr: READ + SHARE my letter to Dr. Kessler, newly named co-chair of Biden’s #COVID-19 Advisory Committee, requesting he imm…',\n",
       " 'RT @ProjectLincoln: RT if you think @Twitter @jack should deactivate @realDonaldTrump’s Twitter account for continuing to spread misinforma…',\n",
       " 'RT @BitSigmaBTC: 【Notice】As my signal of 3 days before. #BTC is near 28000$. \\n\\nCongrats to frds followed me. Big profit in late 2020~\\n\\nNoth…',\n",
       " 'RT @OliviaTroye: A bomb went off on US soil on Christmas morning. People are struggling to feed their families. Everyday people are saying…',\n",
       " 'You will not believe your eyes. https://t.co/dOhI1yZjHX\\n\\n#SCAMDEMIC2020 #ScamdemicIsOver #COVID19 #COVID #covid… https://t.co/yey8XG5Xns']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Tweets without Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_search = search_words + \" -filter:retweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Covid -filter:retweets'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search, q=new_search, lang=\"en\",since=date_since).items(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You will not believe your eyes. https://t.co/dOhI1yZjHX\\n\\n#SCAMDEMIC2020 #ScamdemicIsOver #COVID19 #COVID #covid… https://t.co/yey8XG5Xns',\n",
       " '@DailyMail “The situation is so dire that the top public health official in LA, the country’s most populous county,… https://t.co/y3KPCZR79q',\n",
       " 'The good news - such as it is - about our present #OntarioWide #COVID lockdown is that @jaspermackay and I will not… https://t.co/f58ZLDnWqC',\n",
       " 'We are as conscious and serious about #COVID-19  as you are. Contact today  us to learn more.\\n #naturalremidies… https://t.co/wRROVJUMAb',\n",
       " '@christinemsmit2 @mojos55 \"with\", but not \"from\", but without #covid they would not have died.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get username and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search,q=new_search, lang=\"en\", since=date_since).items(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_locs = [[tweet.user.screen_name,tweet.user.location,tweet.text] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Biblititute',\n",
       "  '',\n",
       "  'You will not believe your eyes. https://t.co/dOhI1yZjHX\\n\\n#SCAMDEMIC2020 #ScamdemicIsOver #COVID19 #COVID #covid… https://t.co/yey8XG5Xns'],\n",
       " ['diggingquote',\n",
       "  'Mars',\n",
       "  '@DailyMail “The situation is so dire that the top public health official in LA, the country’s most populous county,… https://t.co/y3KPCZR79q'],\n",
       " ['MerlinOtello',\n",
       "  'Port Dover, Ontario',\n",
       "  'The good news - such as it is - about our present #OntarioWide #COVID lockdown is that @jaspermackay and I will not… https://t.co/f58ZLDnWqC'],\n",
       " ['NitLynn',\n",
       "  'Georgia, USA',\n",
       "  'We are as conscious and serious about #COVID-19  as you are. Contact today  us to learn more.\\n #naturalremidies… https://t.co/wRROVJUMAb'],\n",
       " ['Sigbertganser',\n",
       "  'France',\n",
       "  '@christinemsmit2 @mojos55 \"with\", but not \"from\", but without #covid they would not have died.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas Dataframe From A List of Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(data=users_locs, columns=['user', \"location\",\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>location</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biblititute</td>\n",
       "      <td></td>\n",
       "      <td>You will not believe your eyes. https://t.co/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diggingquote</td>\n",
       "      <td>Mars</td>\n",
       "      <td>@DailyMail “The situation is so dire that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MerlinOtello</td>\n",
       "      <td>Port Dover, Ontario</td>\n",
       "      <td>The good news - such as it is - about our pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NitLynn</td>\n",
       "      <td>Georgia, USA</td>\n",
       "      <td>We are as conscious and serious about #COVID-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sigbertganser</td>\n",
       "      <td>France</td>\n",
       "      <td>@christinemsmit2 @mojos55 \"with\", but not \"fro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user             location  \\\n",
       "0    Biblititute                        \n",
       "1   diggingquote                 Mars   \n",
       "2   MerlinOtello  Port Dover, Ontario   \n",
       "3        NitLynn         Georgia, USA   \n",
       "4  Sigbertganser               France   \n",
       "\n",
       "                                               tweet  \n",
       "0  You will not believe your eyes. https://t.co/d...  \n",
       "1  @DailyMail “The situation is so dire that the ...  \n",
       "2  The good news - such as it is - about our pres...  \n",
       "3  We are as conscious and serious about #COVID-1...  \n",
       "4  @christinemsmit2 @mojos55 \"with\", but not \"fro...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizaion(text):\n",
    "    return re.split(' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'current',\n",
       " 'pandemic',\n",
       " 'shows',\n",
       " 'that',\n",
       " 'we',\n",
       " 'can',\n",
       " \"'no\",\n",
       " 'longer',\n",
       " 'ignore',\n",
       " 'the',\n",
       " 'risks']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text=\"The current pandemic shows that we can 'no longer ignore the risks\"\n",
    "text_tokenizaion(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With The Twitter Streaming API\n",
    "\n",
    "With Streaming data we need to do the following:\n",
    "1. Create database connection\n",
    "2. Create a persistent connection to the Twitter API, and read each connection incrementally.\n",
    "3. Process tweets quickly, store them in the database, and don’t let your program get backed up.\n",
    "4. Handle errors and other issues properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to MySql Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                           (server,MySQL username,MySQL pass, Database name).\n",
    "connection = MySQLdb.connect(\"localhost\",\"root\",\"root\",\"social_analytic\")\n",
    "cur = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StreamListener Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "search_val=\"covid\"\n",
    "\n",
    "class listener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "       \n",
    "        try:\n",
    "#             tweet_count=0\n",
    "            \n",
    "            all_data = json.loads(data)\n",
    "            \n",
    "#             search_val=\"covid\"\n",
    "            tweet_id=all_data[\"id\"]\n",
    "            created_at=to_datetime(all_data[\"created_at\"])\n",
    "            created_at_date=created_at.date()\n",
    "            created_at_hour=created_at.time().strftime(\"%H\")\n",
    "            created_at_min=created_at.time().strftime(\"%H:%M\")\n",
    "            text = all_data[\"text\"]\n",
    "            source = all_data[\"source\"]\n",
    "#             local_created_at=created_at.str.replace(tzinfo=timezone.utc).astimezone(tz=None)\n",
    "            local_created_at=all_data[\"created_at\"]\n",
    "            \n",
    "            in_reply_to_status_id=all_data['in_reply_to_status_id_str']\n",
    "            in_reply_to_user_id=all_data['in_reply_to_user_id_str']\n",
    "            in_reply_to_screen_name=all_data['in_reply_to_screen_name']\n",
    "            username=all_data['user']['name']\n",
    "            user_screen_name=all_data['user']['screen_name']\n",
    "            user_location=all_data['user']['location']\n",
    "            user_url=all_data['user']['url']\n",
    "            user_description=all_data['user']['description']\n",
    "            user_verified_str=all_data['user']['verified']\n",
    "            user_verified=0\n",
    "            if user_verified_str==True:\n",
    "                user_verified=1\n",
    "            user_followers_count=all_data['user']['followers_count']\n",
    "            user_friends_count=all_data['user']['friends_count']\n",
    "            user_listed_count=all_data['user']['listed_count']\n",
    "            user_favourites_count=all_data['user']['favourites_count']\n",
    "            user_created_at=to_datetime(all_data['user']['created_at'])\n",
    "            user_created_at_date=user_created_at.date()\n",
    "            user_id=all_data['user']['id']\n",
    "            user_profile_image_url_https=all_data['user']['profile_image_url_https']\n",
    "            \n",
    "#             coordinates_lat=all_data['coordinates'][0]\n",
    "#             coordinates_lon=all_data['coordinates'][1]\n",
    "            if all_data['coordinates']==True:\n",
    "                coordinates_lat=all_data['coordinates'][0]\n",
    "                coordinates_lon=all_data['coordinates'][1]\n",
    "            else:\n",
    "                coordinates_lat=None\n",
    "                coordinates_lon=None\n",
    "            if all_data['place']==True:\n",
    "                place_country=all_data['place_country']\n",
    "                place_country_code=all_data['place_country_code']\n",
    "                place_full_name=all_data['place_full_name']\n",
    "                place_id=all_data['place_id']\n",
    "                place_type=all_data['place_type']\n",
    "            else:\n",
    "                place_country=None\n",
    "                place_country_code=None\n",
    "                place_full_name=None\n",
    "                place_id=None\n",
    "                place_type=None\n",
    "                \n",
    "            retweeted_status=None\n",
    "            \n",
    "            if all_data['is_quote_status']==True:\n",
    "                quoted_status_id=all_data['quoted_status_id']\n",
    "                quoted_status=all_data['quoted_status']\n",
    "#                 retweeted_status=all_data['retweeted_status']\n",
    "                quote_count=0\n",
    "            else:\n",
    "                quoted_status_id=None\n",
    "                quote_count=0\n",
    "                quoted_status=None\n",
    "#                 retweeted_status=None\n",
    "            try:\n",
    "                reply_count=all_data['reply_count']\n",
    "            except:\n",
    "                reply_count=0\n",
    "\n",
    "            retweet_count=all_data['retweet_count']\n",
    "            favorite_count=all_data['favorite_count']\n",
    "            retweeted_txt=all_data['retweeted']\n",
    "            entities=str(all_data['entities'])\n",
    "            retweeted=0\n",
    "            if retweeted_txt==True:\n",
    "                retweeted=1\n",
    "#             feeds_link=all_data['https://twitter.com/_/status/'+id]\n",
    "            feeds_link=None\n",
    "            lang=all_data['lang']\n",
    "            feeds_image=None\n",
    "            feeds_video=None\n",
    "            # clean text\n",
    "            text_preprocess = lambda x: re.compile('\\#').sub('', re.compile('RT @').sub('@', x).strip())\n",
    "            clean_text=text_preprocess(text)\n",
    "            clean_text=re.sub('@[^\\s]+','',clean_text)\n",
    "            clean_text=re.sub(r\"http\\S+\", \"\", clean_text)\n",
    "            tokenized_text=text_tokenizaion(clean_text)\n",
    "            # end clean text\n",
    "            text_sentiment_polarity=TextBlob(clean_text).sentiment[0]\n",
    "            text_sentiment_subjectivity=TextBlob(clean_text).sentiment[1]\n",
    "\n",
    "\n",
    "            cur.execute(\"INSERT INTO tweet (search_val,  created_at, created_at_date,created_at_hour,created_at_min, tweet_id,  text, source,in_reply_to_status_id, in_reply_to_user_id,  in_reply_to_screen_name,  user_name,  user_screen_name,  user_location, user_url,  user_description, user_verified, user_followers_count,  user_friends_count, user_listed_count,  user_favourites_count,  user_created_at,user_created_at_date, user_id,  user_profile_image_url_https,  coordinates_lat,  coordinates_lon, place_country,  place_country_code,  place_full_name, place_id,  place_type,  quoted_status_id,  quoted_status, retweeted_status,  quote_count,  reply_count,  retweet_count,  favorite_count, retweeted,  entities,  lang, feeds_link,  feeds_video, feeds_image,clean_text,text_sentiment_polarity,text_sentiment_subjectivity) VALUES (%s, %s,%s, %s,%s, %s,%s,%s,%s, %s, %s, %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\", \n",
    "                        (search_val,created_at,created_at_date,created_at_hour,created_at_min,tweet_id,text,source,in_reply_to_status_id,in_reply_to_user_id,in_reply_to_screen_name,username,user_screen_name,user_location,user_url,user_description,user_verified,user_followers_count,user_friends_count,user_listed_count,user_favourites_count,user_created_at,user_created_at_date,user_id,user_profile_image_url_https,coordinates_lat,coordinates_lon,place_country,place_country_code,place_full_name,place_id,place_type,quoted_status_id,quoted_status,retweeted_status,quote_count,reply_count,retweet_count,favorite_count,retweeted,entities,lang,feeds_link,feeds_video,feeds_image,clean_text,text_sentiment_polarity,text_sentiment_subjectivity))\n",
    "\n",
    "            connection.commit()\n",
    "            print(\"Fetched tweets ...........\\n\")\n",
    "            print(created_at,user_created_at)\n",
    "#             tweet_count +=1\n",
    "\n",
    "            return True\n",
    "\n",
    "# Catch all the errors from Database or Twiteer\n",
    "        except tweepy.error.TweepError:\n",
    "            print(\"Error fetching the data\")\n",
    "            \n",
    "        except UnicodeEncodeError:\n",
    "            pass\n",
    "        \n",
    "        except MySQLdb.DataError as e:\n",
    "            print(\"DataError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.InternalError as e:\n",
    "            print(\"InternalError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.IntegrityError as e:\n",
    "            print(\"IntegrityError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.OperationalError as e:\n",
    "            print(\"OperationalError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.NotSupportedError as e:\n",
    "            print(\"NotSupportedError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.ProgrammingError as e:\n",
    "            print(\"ProgrammingError\")\n",
    "            print(e)\n",
    "        except:\n",
    "            print (\"Unexpected general error: \", sys.exc_info()[0])\n",
    "            raise\n",
    "        finally:\n",
    "            print(\"  \")\n",
    "            #             cur.close()\n",
    "            #             connection.close()\n",
    "            \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2018-02-20 03:45:29\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2012-05-29 23:39:13\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2018-03-02 10:23:40\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2010-12-17 14:02:00\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2009-06-11 20:42:19\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2013-01-06 20:35:52\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2020-11-01 03:44:59\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2020-09-10 08:45:46\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2019-12-11 15:11:35\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2018-04-01 01:33:00\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2013-02-02 08:01:28\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2010-09-12 13:09:27\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2011-04-14 13:13:32\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2013-05-08 17:26:03\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2020-11-18 04:49:40\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2020-06-27 06:46:48\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:05 2014-08-04 14:44:08\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2020-08-12 06:17:16\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2016-07-23 07:17:12\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2019-09-22 20:12:25\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2017-02-07 09:33:30\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2013-10-18 23:55:47\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2019-07-26 17:39:26\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2019-01-17 23:18:15\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2020-11-07 20:03:22\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2013-10-17 17:23:34\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2017-12-15 14:29:36\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2019-11-08 01:14:50\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2011-05-15 16:49:26\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:06 2009-08-17 17:42:27\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2020-09-10 21:19:06\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2011-08-10 09:30:15\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2009-12-19 08:09:57\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2020-03-09 16:57:50\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2020-12-22 14:51:58\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2015-03-15 02:43:56\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2013-03-24 13:24:19\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2020-12-20 13:34:49\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:07 2014-03-18 01:26:24\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2011-09-23 13:47:34\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2012-09-15 20:44:11\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2020-11-19 03:26:43\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2016-04-08 07:02:39\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2020-03-04 11:28:05\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2011-04-01 22:05:36\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2016-01-31 15:08:57\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2020-05-30 02:52:37\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2019-01-10 15:45:07\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2010-07-12 03:51:19\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2015-04-28 17:00:26\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2019-03-10 23:23:09\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:08 2010-03-28 16:18:32\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2012-05-31 13:12:28\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2020-10-08 11:31:35\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2016-10-07 11:50:51\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2016-10-06 16:57:52\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2014-08-06 01:05:39\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2016-01-23 16:48:29\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2012-11-25 12:50:43\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:09 2016-08-06 16:55:30\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2015-05-01 01:41:58\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2008-10-28 01:52:36\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2015-11-23 02:56:22\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2013-08-01 23:25:43\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2010-05-28 19:56:02\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2014-01-13 01:40:52\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2016-01-07 20:49:20\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2017-10-24 17:26:34\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:10 2014-01-03 08:24:40\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2014-06-11 17:22:06\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2015-01-06 19:15:14\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2011-10-14 21:14:57\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2008-11-17 20:54:18\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2017-01-06 20:09:33\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2017-07-08 18:18:07\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2010-08-25 19:06:41\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2018-02-14 23:51:53\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2014-08-23 22:06:31\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2020-07-22 18:12:23\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:11 2009-03-26 16:49:06\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:12 2018-06-01 21:32:31\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:12 2020-11-20 23:04:11\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-27 15:54:12 2012-12-11 06:07:34\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n"
     ]
    },
    {
     "ename": "ProtocolError",
     "evalue": "('Connection broken: IncompleteRead(7623 bytes read)', IncompleteRead(7623 bytes read))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mchunk_left\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_readinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtotal_bytes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m             \u001b[0mmvb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmvb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(0 bytes read, 3774 more expected)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtotal_bytes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(7623 bytes read)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9c053ae98fbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtwitterStream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtwitterStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'covid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlanguages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstall_warnings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filter_level'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'delimited'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'length'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_start\u001b[1;34m(self, is_async)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[1;31m# call a handler first so that the exception can be logged.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[1;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[1;34m(self, resp)\u001b[0m\n\u001b[0;32m    347\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expecting length, unexpected value found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m             \u001b[0mnext_status_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnext_status_obj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_status_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mread_len\u001b[1;34m(self, length)\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mread_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    399\u001b[0m                         \u001b[1;31m# raised during streaming, so all calls with incorrect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m                         \u001b[1;31m# Content-Length are caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength_remaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mHTTPException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[1;31m# This includes IncompleteRead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Connection broken: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;31m# If no exception is thrown, we should avoid cleaning up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(7623 bytes read)', IncompleteRead(7623 bytes read))"
     ]
    }
   ],
   "source": [
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track=['covid'],languages = [\"en\"], stall_warnings = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
