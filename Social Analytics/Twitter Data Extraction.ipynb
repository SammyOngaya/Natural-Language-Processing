{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the tweepy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy\n",
    "# !pip install mysqlclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import MySQLdb\n",
    "\n",
    "\n",
    "# For sentiment analysis of tweets\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tweeter Consumer Keys and Access Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'NFIO7bIgwQpdCZ7p7rgewKXss'\n",
    "consumer_secret= '2QegNrIpcGCW6nCWFTL5JEsrDCmYZpQHJ8SdIT7V0ql5cQgFwV'\n",
    "access_token= '3049227557-36ln3qak9xbxHvgNos45cf3uScGxuUdaDAR4vqs'\n",
    "access_token_secret= 'pZ4Y0p31graKrAYAy41mYztvIrYFzof7awiTDWQr2ui9u'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize AuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define search words and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"#Covid\"\n",
    "date_since = \"2018-11-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search, q=search_words, lang=\"en\", since=date_since).items(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tweepy.cursor.ItemIterator at 0x17be288a358>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through tweets using for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 17:03:03\n",
      "2020-12-26 18:01:29\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from email.utils import parsedate_tz\n",
    "\n",
    "s='Mon Dec 07 17:03:03 +0000 2020'\n",
    "def to_datetime(datestring):\n",
    "    time_tuple = parsedate_tz(datestring.strip())\n",
    "    dt = datetime(*time_tuple[:6])\n",
    "    proper_date=(dt - timedelta(seconds=time_tuple[-1]))\n",
    "    return proper_date\n",
    "\n",
    "print(to_datetime(s))\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve tweets as a collection of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beware of scams for COVID #Vaccines or tests! #COVID vaccines are free for everyone, and COVID tests are free for M… https://t.co/RKtjIZmJuz',\n",
       " 'RT @AllergyHealth: I just got my 1st dose of #Coronavirus #vaccine &amp; I am halfway to protected against severe #COVID-19 disease. I’m still…',\n",
       " '\"Like many in the #restaurant #industry, Walter Green has had a tough year — but after getting diagnosed with… https://t.co/AQ37fMVduj',\n",
       " 'RT @AgentP22: Anyone else looking forward to the Ian Blackford tweet demanding that #covid rule breaker Nicola Sturgeon resign?\\n\\n#ResignStu…',\n",
       " 'RT @CAChirag: Income Tax E Filing portal not working - have to login 10 to 15 times to upload one ITR \\n\\nComplete waste of Man-hours during…']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Tweets without Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_search = search_words + \" -filter:retweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Covid -filter:retweets'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search, q=new_search, lang=\"en\",since=date_since).items(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beware of scams for COVID #Vaccines or tests! #COVID vaccines are free for everyone, and COVID tests are free for M… https://t.co/RKtjIZmJuz',\n",
       " '\"Like many in the #restaurant #industry, Walter Green has had a tough year — but after getting diagnosed with… https://t.co/AQ37fMVduj',\n",
       " '#Covid #Vaccines #Likely #Effective #Against new #Strains, #Doctor says\\nhttps://t.co/RciRkWN3wH https://t.co/R8IGdBr01z',\n",
       " 'A tragic love story in the form of a picture book for adults. You have to read it!\\nhttps://t.co/UaIiNxT54d\\n#covid… https://t.co/hYmGYS0Nat',\n",
       " 'It’s Been 48 Hours Since My COVID Vaccine. This is How I Feel: https://t.co/o6FaWjKHJl. #covid #covidvaccine']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get username and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search,q=new_search, lang=\"en\", since=date_since).items(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_locs = [[tweet.user.screen_name,tweet.user.location,tweet.text] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stopcancerfund',\n",
       "  'Washington, D.C.',\n",
       "  'Beware of scams for COVID #Vaccines or tests! #COVID vaccines are free for everyone, and COVID tests are free for M… https://t.co/RKtjIZmJuz'],\n",
       " ['ALCOSales',\n",
       "  'Burr Ridge, IL',\n",
       "  '\"Like many in the #restaurant #industry, Walter Green has had a tough year — but after getting diagnosed with… https://t.co/AQ37fMVduj'],\n",
       " ['BloGoalcom',\n",
       "  '',\n",
       "  '#Covid #Vaccines #Likely #Effective #Against new #Strains, #Doctor says\\nhttps://t.co/RciRkWN3wH https://t.co/R8IGdBr01z'],\n",
       " ['OrBragi',\n",
       "  'Rustenburg, South Africa',\n",
       "  'A tragic love story in the form of a picture book for adults. You have to read it!\\nhttps://t.co/UaIiNxT54d\\n#covid… https://t.co/hYmGYS0Nat'],\n",
       " ['DrJenCaudle',\n",
       "  'Philadelphia, PA',\n",
       "  'It’s Been 48 Hours Since My COVID Vaccine. This is How I Feel: https://t.co/o6FaWjKHJl. #covid #covidvaccine']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas Dataframe From A List of Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(data=users_locs, columns=['user', \"location\",\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>location</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stopcancerfund</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>Beware of scams for COVID #Vaccines or tests! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALCOSales</td>\n",
       "      <td>Burr Ridge, IL</td>\n",
       "      <td>\"Like many in the #restaurant #industry, Walte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BloGoalcom</td>\n",
       "      <td></td>\n",
       "      <td>#Covid #Vaccines #Likely #Effective #Against n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OrBragi</td>\n",
       "      <td>Rustenburg, South Africa</td>\n",
       "      <td>A tragic love story in the form of a picture b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DrJenCaudle</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>It’s Been 48 Hours Since My COVID Vaccine. Thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user                  location  \\\n",
       "0  stopcancerfund          Washington, D.C.   \n",
       "1       ALCOSales            Burr Ridge, IL   \n",
       "2      BloGoalcom                             \n",
       "3         OrBragi  Rustenburg, South Africa   \n",
       "4     DrJenCaudle          Philadelphia, PA   \n",
       "\n",
       "                                               tweet  \n",
       "0  Beware of scams for COVID #Vaccines or tests! ...  \n",
       "1  \"Like many in the #restaurant #industry, Walte...  \n",
       "2  #Covid #Vaccines #Likely #Effective #Against n...  \n",
       "3  A tragic love story in the form of a picture b...  \n",
       "4  It’s Been 48 Hours Since My COVID Vaccine. Thi...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With The Twitter Streaming API\n",
    "\n",
    "With Streaming data we need to do the following:\n",
    "1. Create database connection\n",
    "2. Create a persistent connection to the Twitter API, and read each connection incrementally.\n",
    "3. Process tweets quickly, store them in the database, and don’t let your program get backed up.\n",
    "4. Handle errors and other issues properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to MySql Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                           (server,MySQL username,MySQL pass, Database name).\n",
    "connection = MySQLdb.connect(\"localhost\",\"root\",\"root\",\"social_analytic\")\n",
    "cur = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StreamListener Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "search_val=\"covid\"\n",
    "\n",
    "class listener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "       \n",
    "        try:\n",
    "#             tweet_count=0\n",
    "            \n",
    "            all_data = json.loads(data)\n",
    "            \n",
    "#             search_val=\"covid\"\n",
    "            tweet_id=all_data[\"id\"]\n",
    "            created_at=to_datetime(all_data[\"created_at\"])\n",
    "            created_at_date=created_at.date()\n",
    "            created_at_hour=created_at.time().strftime(\"%H\")\n",
    "            created_at_min=created_at.time().strftime(\"%H:%M\")\n",
    "            text = all_data[\"text\"]\n",
    "            source = all_data[\"source\"]\n",
    "#             local_created_at=created_at.str.replace(tzinfo=timezone.utc).astimezone(tz=None)\n",
    "            local_created_at=all_data[\"created_at\"]\n",
    "            \n",
    "            in_reply_to_status_id=all_data['in_reply_to_status_id_str']\n",
    "            in_reply_to_user_id=all_data['in_reply_to_user_id_str']\n",
    "            in_reply_to_screen_name=all_data['in_reply_to_screen_name']\n",
    "            username=all_data['user']['name']\n",
    "            user_screen_name=all_data['user']['screen_name']\n",
    "            user_location=all_data['user']['location']\n",
    "            user_url=all_data['user']['url']\n",
    "            user_description=all_data['user']['description']\n",
    "            user_verified_str=all_data['user']['verified']\n",
    "            user_verified=0\n",
    "            if user_verified_str==True:\n",
    "                user_verified=1\n",
    "            user_followers_count=all_data['user']['followers_count']\n",
    "            user_friends_count=all_data['user']['friends_count']\n",
    "            user_listed_count=all_data['user']['listed_count']\n",
    "            user_favourites_count=all_data['user']['favourites_count']\n",
    "            user_created_at=to_datetime(all_data['user']['created_at'])\n",
    "            user_created_at_date=user_created_at.date()\n",
    "            user_id=all_data['user']['id']\n",
    "            user_profile_image_url_https=all_data['user']['profile_image_url_https']\n",
    "            \n",
    "#             coordinates_lat=all_data['coordinates'][0]\n",
    "#             coordinates_lon=all_data['coordinates'][1]\n",
    "            if all_data['coordinates']==True:\n",
    "                coordinates_lat=all_data['coordinates'][0]\n",
    "                coordinates_lon=all_data['coordinates'][1]\n",
    "            else:\n",
    "                coordinates_lat=None\n",
    "                coordinates_lon=None\n",
    "            if all_data['place']==True:\n",
    "                place_country=all_data['place_country']\n",
    "                place_country_code=all_data['place_country_code']\n",
    "                place_full_name=all_data['place_full_name']\n",
    "                place_id=all_data['place_id']\n",
    "                place_type=all_data['place_type']\n",
    "            else:\n",
    "                place_country=None\n",
    "                place_country_code=None\n",
    "                place_full_name=None\n",
    "                place_id=None\n",
    "                place_type=None\n",
    "                \n",
    "            retweeted_status=None\n",
    "            \n",
    "            if all_data['is_quote_status']==True:\n",
    "                quoted_status_id=all_data['quoted_status_id']\n",
    "                quoted_status=all_data['quoted_status']\n",
    "#                 retweeted_status=all_data['retweeted_status']\n",
    "                quote_count=0\n",
    "            else:\n",
    "                quoted_status_id=None\n",
    "                quote_count=0\n",
    "                quoted_status=None\n",
    "#                 retweeted_status=None\n",
    "            try:\n",
    "                reply_count=all_data['reply_count']\n",
    "            except:\n",
    "                reply_count=0\n",
    "\n",
    "            retweet_count=all_data['retweet_count']\n",
    "            favorite_count=all_data['favorite_count']\n",
    "            retweeted_txt=all_data['retweeted']\n",
    "            entities=str(all_data['entities'])\n",
    "            retweeted=0\n",
    "            if retweeted_txt==True:\n",
    "                retweeted=1\n",
    "#             feeds_link=all_data['https://twitter.com/_/status/'+id]\n",
    "            feeds_link=None\n",
    "            lang=all_data['lang']\n",
    "            feeds_image=None\n",
    "            feeds_video=None\n",
    "            # clean text\n",
    "            text_preprocess = lambda x: re.compile('\\#').sub('', re.compile('RT @').sub('@', x).strip())\n",
    "            clean_text=text_preprocess(text)\n",
    "            clean_text=re.sub('@[^\\s]+','',clean_text)\n",
    "            clean_text=re.sub(r\"http\\S+\", \"\", clean_text)\n",
    "            # end clean text\n",
    "            text_sentiment_polarity=TextBlob(clean_text).sentiment[0]\n",
    "            text_sentiment_subjectivity=TextBlob(clean_text).sentiment[1]\n",
    "\n",
    "\n",
    "            cur.execute(\"INSERT INTO tweet (search_val,  created_at, created_at_date,created_at_hour,created_at_min, tweet_id,  text, source,in_reply_to_status_id, in_reply_to_user_id,  in_reply_to_screen_name,  user_name,  user_screen_name,  user_location, user_url,  user_description, user_verified, user_followers_count,  user_friends_count, user_listed_count,  user_favourites_count,  user_created_at,user_created_at_date, user_id,  user_profile_image_url_https,  coordinates_lat,  coordinates_lon, place_country,  place_country_code,  place_full_name, place_id,  place_type,  quoted_status_id,  quoted_status, retweeted_status,  quote_count,  reply_count,  retweet_count,  favorite_count, retweeted,  entities,  lang, feeds_link,  feeds_video, feeds_image,clean_text,text_sentiment_polarity,text_sentiment_subjectivity) VALUES (%s, %s,%s, %s,%s, %s,%s,%s,%s, %s, %s, %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\", \n",
    "                        (search_val,created_at,created_at_date,created_at_hour,created_at_min,tweet_id,text,source,in_reply_to_status_id,in_reply_to_user_id,in_reply_to_screen_name,username,user_screen_name,user_location,user_url,user_description,user_verified,user_followers_count,user_friends_count,user_listed_count,user_favourites_count,user_created_at,user_created_at_date,user_id,user_profile_image_url_https,coordinates_lat,coordinates_lon,place_country,place_country_code,place_full_name,place_id,place_type,quoted_status_id,quoted_status,retweeted_status,quote_count,reply_count,retweet_count,favorite_count,retweeted,entities,lang,feeds_link,feeds_video,feeds_image,clean_text,text_sentiment_polarity,text_sentiment_subjectivity))\n",
    "\n",
    "            connection.commit()\n",
    "            print(\"Fetched tweets ...........\\n\")\n",
    "            print(created_at,user_created_at)\n",
    "#             tweet_count +=1\n",
    "\n",
    "            return True\n",
    "\n",
    "# Catch all the errors from Database or Twiteer\n",
    "        except tweepy.error.TweepError:\n",
    "            print(\"Error fetching the data\")\n",
    "            \n",
    "        except UnicodeEncodeError:\n",
    "            pass\n",
    "        \n",
    "        except MySQLdb.DataError as e:\n",
    "            print(\"DataError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.InternalError as e:\n",
    "            print(\"InternalError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.IntegrityError as e:\n",
    "            print(\"IntegrityError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.OperationalError as e:\n",
    "            print(\"OperationalError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.NotSupportedError as e:\n",
    "            print(\"NotSupportedError\")\n",
    "            print(e)\n",
    "\n",
    "        except MySQLdb.ProgrammingError as e:\n",
    "            print(\"ProgrammingError\")\n",
    "            print(e)\n",
    "        except:\n",
    "            print (\"Unexpected general error: \", sys.exc_info()[0])\n",
    "            raise\n",
    "        finally:\n",
    "            print(\"  \")\n",
    "            #             cur.close()\n",
    "            #             connection.close()\n",
    "            \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:31 2009-08-16 00:20:54\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:31 2016-02-11 11:46:16\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:31 2016-10-18 17:57:14\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:31 2009-03-26 17:56:43\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:31 2019-04-25 08:09:33\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2017-03-15 21:26:21\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2017-12-01 23:46:06\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2020-12-26 14:57:27\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2017-02-01 00:20:49\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2016-09-27 23:48:20\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2009-07-09 06:36:54\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2017-08-04 00:42:08\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2017-08-15 16:25:35\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2020-08-21 12:41:48\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2009-05-06 01:25:19\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2020-11-20 02:52:01\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2020-09-29 12:44:05\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2014-04-06 17:25:46\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:32 2013-06-17 22:19:09\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:33 2016-10-11 00:57:57\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:33 2018-08-18 23:35:55\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:33 2019-11-25 03:10:26\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:33 2010-11-25 23:05:37\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:33 2020-11-19 00:22:19\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2016-06-30 00:19:29\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2019-08-02 22:39:59\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2016-09-26 15:23:01\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2018-02-11 23:21:58\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2012-12-05 21:19:50\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2014-08-05 23:09:18\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2020-11-23 11:59:45\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:34 2013-01-25 10:13:08\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:35 2020-12-04 02:14:17\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:35 2009-03-17 15:57:55\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:35 2015-06-01 06:37:23\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:35 2016-09-27 03:02:26\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:35 2014-08-02 15:42:21\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:35 2018-07-09 07:28:15\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2020-12-21 19:36:18\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2013-11-07 16:52:27\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2020-07-29 21:18:38\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2013-12-09 11:27:09\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2017-06-11 07:23:57\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2019-03-13 13:29:44\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2015-10-16 00:50:04\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2018-02-19 13:12:00\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2011-03-01 19:33:16\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:36 2009-03-09 23:37:09\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2007-12-02 23:38:07\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2020-06-10 23:59:56\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2012-01-23 22:42:05\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2017-10-19 15:27:32\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2012-03-11 12:54:37\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2014-09-20 01:42:36\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2008-07-17 22:21:44\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2009-12-20 15:20:46\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:37 2013-07-28 20:24:03\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:38 2009-06-29 03:58:50\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:38 2012-05-23 23:18:27\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2009-06-19 18:51:08\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2008-08-21 21:58:24\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2009-06-01 12:55:28\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2011-11-14 19:40:02\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2019-10-20 19:56:25\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2014-06-12 14:51:03\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2009-04-30 17:58:18\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2009-02-26 17:04:24\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2008-04-29 03:44:20\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2018-09-15 17:05:04\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2018-10-12 13:15:53\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:39 2020-03-29 16:59:39\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2010-01-29 21:20:38\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2011-09-07 10:20:43\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2020-08-16 15:43:05\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2016-04-20 01:27:56\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2007-04-26 01:19:49\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2015-03-10 22:16:21\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2009-02-04 12:48:37\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:40 2018-11-02 15:17:54\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2012-02-06 02:08:14\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2010-03-16 15:32:38\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2018-11-03 10:28:32\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2020-05-29 15:43:59\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2016-11-13 21:01:25\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2013-02-17 01:18:00\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2011-06-05 02:25:31\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2017-05-28 10:16:36\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:41 2011-10-12 05:59:39\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:42 2010-05-26 06:26:12\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:42 2017-08-01 17:14:00\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:42 2008-12-10 22:38:54\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:42 2010-09-11 12:34:37\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:42 2017-03-15 21:26:21\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:42 2011-02-24 01:17:35\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:43 2009-06-17 14:03:39\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:43 2011-10-06 00:37:00\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2013-03-06 23:07:23\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2018-03-15 05:24:24\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2019-07-04 15:51:30\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2016-12-10 16:16:47\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2009-05-22 13:35:21\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2017-10-18 11:19:03\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2009-09-04 18:47:30\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2015-05-28 17:52:34\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:44 2019-12-05 00:19:02\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:45 2009-01-21 22:39:12\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:45 2009-04-30 18:49:34\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:45 2009-01-23 21:24:41\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:45 2010-06-25 19:00:41\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:45 2009-01-23 21:37:36\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:45 2010-07-06 22:41:41\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:45 2018-05-10 16:57:43\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:46 2012-05-14 17:52:03\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:46 2010-12-21 13:53:16\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:46 2018-05-11 11:41:45\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:46 2012-07-18 20:37:50\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:46 2018-01-21 19:47:20\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:46 2019-07-04 17:21:19\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:46 2008-12-10 20:23:57\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:47 2011-06-03 10:02:06\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:47 2017-07-29 19:40:31\n",
      "  \n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:47 2012-01-09 23:23:50\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:47 2014-07-23 08:21:51\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:47 2013-02-08 20:56:47\n",
      "  \n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:47 2018-05-13 01:35:17\n",
      "  \n",
      "Fetched tweets ...........\n",
      "\n",
      "2020-12-26 15:01:47 2013-06-25 01:25:59\n",
      "  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track=['covid'],languages = [\"en\"], stall_warnings = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
